{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"prefect-dask - Coordinate, and parallelize, your dataflow","text":"<p>The <code>prefect-dask</code> collection makes it easy to include distributed processing for your flows. Check out the examples below to get started!</p>"},{"location":"#integrate-with-prefect-flows","title":"Integrate with Prefect flows","text":"<p>Perhaps you're already working with Prefect flows. Say your flow downloads many images to train your machine learning model. Unfortunately, it takes a long time to download your flows because your code is running sequentially.</p> <p>After installing <code>prefect-dask</code> you can parallelize your flow in three simple steps:</p> <ol> <li>Add the import: <code>from prefect_dask import DaskTaskRunner</code></li> <li>Specify the task runner in the flow decorator: <code>@flow(task_runner=DaskTaskRunner)</code></li> <li>Submit tasks to the flow's task runner: <code>a_task.submit(*args, **kwargs)</code> The parallelized code  runs in about 1/3 of the time in our test!  And that's without distributing the workload over multiple machines. Here's the before and after!</li> </ol> BeforeAfter <pre><code># Completed in 15.2 seconds\nfrom typing import List\nfrom pathlib import Path\n\nimport httpx\nfrom prefect import flow, task\n\n\nURL_FORMAT = (\n    \"https://www.cpc.ncep.noaa.gov/products/NMME/archive/\"\n    \"{year:04d}{month:02d}0800/current/images/nino34.rescaling.ENSMEAN.png\"\n)\n\n@task\ndef download_image(year: int, month: int, directory: Path) -&gt; Path:\n    # download image from URL\n    url = URL_FORMAT.format(year=year, month=month)\n    resp = httpx.get(url)\n\n    # save content to directory/YYYYMM.png\n    file_path = (directory / url.split(\"/\")[-1]).with_stem(f\"{year:04d}{month:02d}\")\n    file_path.write_bytes(resp.content)\n    return file_path\n\n@flow\ndef download_nino_34_plumes_from_year(year: int) -&gt; List[Path]:\n    # create a directory to hold images\n    directory = Path(\"data\")\n    directory.mkdir(exist_ok=True)\n\n    # download all images\n    file_paths = []\n    for month in range(1, 12 + 1):\n        file_path = download_image(year, month, directory)\n        file_paths.append(file_path)\n    return file_paths\n\nif __name__ == \"__main__\":\n    download_nino_34_plumes_from_year(2022)\n</code></pre> <pre><code># Completed in 5.7 seconds\nfrom typing import List\nfrom pathlib import Path\n\nimport httpx\nfrom prefect import flow, task\nfrom prefect_dask import DaskTaskRunner\nURL_FORMAT = (\n    \"https://www.cpc.ncep.noaa.gov/products/NMME/archive/\"\n    \"{year:04d}{month:02d}0800/current/images/nino34.rescaling.ENSMEAN.png\"\n)\n\n@task\ndef download_image(year: int, month: int, directory: Path) -&gt; Path:\n    # download image from URL\n    url = URL_FORMAT.format(year=year, month=month)\n    resp = httpx.get(url)\n\n    # save content to directory/YYYYMM.png\n    file_path = (directory / url.split(\"/\")[-1]).with_stem(f\"{year:04d}{month:02d}\")\n    file_path.write_bytes(resp.content)\n    return file_path\n\n@flow(task_runner=DaskTaskRunner(cluster_kwargs={\"processes\": False}))\ndef download_nino_34_plumes_from_year(year: int) -&gt; List[Path]:\n    # create a directory to hold images\n    directory = Path(\"data\")\n    directory.mkdir(exist_ok=True)\n\n    # download all images\n    file_paths = []\n    for month in range(1, 12 + 1):\nfile_path = download_image.submit(year, month, directory)\nfile_paths.append(file_path)\n    return file_paths\n\nif __name__ == \"__main__\":\n    download_nino_34_plumes_from_year(2022)\n</code></pre> <p>The original flow completes in 15.2 seconds.</p> <p>However, with just a few minor tweaks, we were able to reduce the runtime by nearly three folds, down to just 5.7 seconds!</p>"},{"location":"#integrate-with-dask-clientcluster-and-collections","title":"Integrate with Dask client/cluster and collections","text":"<p>Suppose you have an existing Dask client/cluster and collection, like a <code>dask.dataframe.DataFrame</code>, and you want to add observability.</p> <p>With <code>prefect-dask</code>, there's no major overhaul necessary because Prefect was designed with incremental adoption in mind! It's as easy as:</p> <ol> <li>Adding the imports</li> <li>Sprinkling a few <code>task</code> and <code>flow</code> decorators</li> <li>Using <code>get_dask_client</code> context manager on collections to distribute work across workers</li> <li>Specifying the task runner and client's address in the flow decorator</li> <li>Submitting the tasks to the flow's task runner</li> </ol> BeforeAfter <pre><code>import dask.dataframe\nimport dask.distributed\n\n\n\nclient = dask.distributed.Client()\n\n\ndef read_data(start: str, end: str) -&gt; dask.dataframe.DataFrame:\n    df = dask.datasets.timeseries(start, end, partition_freq=\"4w\")\n    return df\n\n\ndef process_data(df: dask.dataframe.DataFrame) -&gt; dask.dataframe.DataFrame:\n    df_yearly_avg = df.groupby(df.index.year).mean()\n    return df_yearly_avg.compute()\n\n\ndef dask_pipeline():\n    df = read_data(\"1988\", \"2022\")\n    df_yearly_average = process_data(df)\n    return df_yearly_average\n\ndask_pipeline()\n</code></pre> <pre><code>import dask.dataframe\nimport dask.distributed\nfrom prefect import flow, task\nfrom prefect_dask import DaskTaskRunner, get_dask_client\nclient = dask.distributed.Client()\n\n@task\ndef read_data(start: str, end: str) -&gt; dask.dataframe.DataFrame:\n    df = dask.datasets.timeseries(start, end, partition_freq=\"4w\")\n    return df\n\n@task\ndef process_data(df: dask.dataframe.DataFrame) -&gt; dask.dataframe.DataFrame:\nwith get_dask_client():\ndf_yearly_avg = df.groupby(df.index.year).mean()\n        return df_yearly_avg.compute()\n\n@flow(task_runner=DaskTaskRunner(address=client.scheduler.address))\ndef dask_pipeline():\ndf = read_data.submit(\"1988\", \"2022\")\ndf_yearly_average = process_data.submit(df)\nreturn df_yearly_average\n\ndask_pipeline()\n</code></pre> <p>Now, you can conveniently see when each task completed, both in the terminal and the UI!</p> <pre><code>14:10:09.845 | INFO    | prefect.engine - Created flow run 'chocolate-pony' for flow 'dask-flow'\n14:10:09.847 | INFO    | prefect.task_runner.dask - Connecting to an existing Dask cluster at tcp://127.0.0.1:59255\n14:10:09.857 | INFO    | distributed.scheduler - Receive client connection: Client-8c1e0f24-9133-11ed-800e-86f2469c4e7a\n14:10:09.859 | INFO    | distributed.core - Starting established connection to tcp://127.0.0.1:59516\n14:10:09.862 | INFO    | prefect.task_runner.dask - The Dask dashboard is available at http://127.0.0.1:8787/status\n14:10:11.344 | INFO    | Flow run 'chocolate-pony' - Created task run 'read_data-5bc97744-0' for task 'read_data'\n14:10:11.626 | INFO    | Flow run 'chocolate-pony' - Submitted task run 'read_data-5bc97744-0' for execution.\n14:10:11.795 | INFO    | Flow run 'chocolate-pony' - Created task run 'process_data-090555ba-0' for task 'process_data'\n14:10:11.798 | INFO    | Flow run 'chocolate-pony' - Submitted task run 'process_data-090555ba-0' for execution.\n14:10:13.279 | INFO    | Task run 'read_data-5bc97744-0' - Finished in state Completed()\n14:11:43.539 | INFO    | Task run 'process_data-090555ba-0' - Finished in state Completed()\n14:11:43.883 | INFO    | Flow run 'chocolate-pony' - Finished in state Completed('All states completed.')\n</code></pre>"},{"location":"#resources","title":"Resources","text":"<p>For additional examples, check out the Usage Guide!</p>"},{"location":"#installation","title":"Installation","text":"<p>Get started by installing <code>prefect-dask</code>!</p> pipconda <pre><code>pip install -U prefect-dask\n</code></pre> <pre><code>conda install -c conda-forge prefect-dask\n</code></pre> <p>Requires an installation of Python 3.7+.</p> <p>We recommend using a Python virtual environment manager such as pipenv, conda, or virtualenv.</p> <p>These tasks are designed to work with Prefect 2. For more information about how to use Prefect, please refer to the Prefect documentation.</p>"},{"location":"#feedback","title":"Feedback","text":"<p>If you encounter any bugs while using <code>prefect-dask</code>, feel free to open an issue in the prefect-dask repository.</p> <p>If you have any questions or issues while using <code>prefect-dask</code>, you can find help in either the Prefect Discourse forum or the Prefect Slack community.</p> <p>Feel free to star or watch <code>prefect-dask</code> for updates too!</p>"},{"location":"#contributing","title":"Contributing","text":"<p>If you'd like to help contribute to fix an issue or add a feature to <code>prefect-dask</code>, please propose changes through a pull request from a fork of the repository.</p> <p>Here are the steps:</p> <ol> <li>Fork the repository</li> <li>Clone the forked repository</li> <li>Install the repository and its dependencies: <pre><code>pip install -e \".[dev]\"\n</code></pre></li> <li>Make desired changes</li> <li>Add tests</li> <li>Insert an entry to CHANGELOG.md</li> <li>Install <code>pre-commit</code> to perform quality checks prior to commit: <pre><code>pre-commit install\n</code></pre></li> <li><code>git commit</code>, <code>git push</code>, and create a pull request</li> </ol>"},{"location":"task_runners/","title":"Task Runners","text":""},{"location":"task_runners/#prefect_dask.task_runners","title":"<code>prefect_dask.task_runners</code>","text":"<p>Interface and implementations of the Dask Task Runner. Task Runners in Prefect are responsible for managing the execution of Prefect task runs. Generally speaking, users are not expected to interact with task runners outside of configuring and initializing them for a flow.</p> Example <pre><code>import time\n\nfrom prefect import flow, task\n\n@task\ndef shout(number):\n    time.sleep(0.5)\n    print(f\"#{number}\")\n\n@flow\ndef count_to(highest_number):\n    for number in range(highest_number):\n        shout.submit(number)\n\nif __name__ == \"__main__\":\n    count_to(10)\n\n# outputs\n#0\n#1\n#2\n#3\n#4\n#5\n#6\n#7\n#8\n#9\n</code></pre> <p>Switching to a <code>DaskTaskRunner</code>: <pre><code>import time\n\nfrom prefect import flow, task\nfrom prefect_dask import DaskTaskRunner\n\n@task\ndef shout(number):\n    time.sleep(0.5)\n    print(f\"#{number}\")\n\n@flow(task_runner=DaskTaskRunner)\ndef count_to(highest_number):\n    for number in range(highest_number):\n        shout.submit(number)\n\nif __name__ == \"__main__\":\n    count_to(10)\n\n# outputs\n#3\n#7\n#2\n#6\n#4\n#0\n#1\n#5\n#8\n#9\n</code></pre></p>"},{"location":"task_runners/#prefect_dask.task_runners-classes","title":"Classes","text":""},{"location":"task_runners/#prefect_dask.task_runners.DaskTaskRunner","title":"<code>DaskTaskRunner</code>","text":"<p>         Bases: <code>BaseTaskRunner</code></p> <p>A parallel task_runner that submits tasks to the <code>dask.distributed</code> scheduler. By default a temporary <code>distributed.LocalCluster</code> is created (and subsequently torn down) within the <code>start()</code> contextmanager. To use a different cluster class (e.g. <code>dask_kubernetes.KubeCluster</code>), you can specify <code>cluster_class</code>/<code>cluster_kwargs</code>.</p> <p>Alternatively, if you already have a dask cluster running, you can provide the address of the scheduler via the <code>address</code> kwarg.</p> <p>Multiprocessing safety</p> <p>Note that, because the <code>DaskTaskRunner</code> uses multiprocessing, calls to flows in scripts must be guarded with <code>if __name__ == \"__main__\":</code> or warnings will be displayed.</p> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>string</code> <p>Address of a currently running dask scheduler; if one is not provided, a temporary cluster will be created in <code>DaskTaskRunner.start()</code>.  Defaults to <code>None</code>.</p> <code>None</code> <code>cluster_class</code> <code>string or callable</code> <p>The cluster class to use when creating a temporary dask cluster. Can be either the full class name (e.g. <code>\"distributed.LocalCluster\"</code>), or the class itself.</p> <code>None</code> <code>cluster_kwargs</code> <code>dict</code> <p>Additional kwargs to pass to the <code>cluster_class</code> when creating a temporary dask cluster.</p> <code>None</code> <code>adapt_kwargs</code> <code>dict</code> <p>Additional kwargs to pass to <code>cluster.adapt</code> when creating a temporary dask cluster. Note that adaptive scaling is only enabled if <code>adapt_kwargs</code> are provided.</p> <code>None</code> <code>client_kwargs</code> <code>dict</code> <p>Additional kwargs to use when creating a <code>dask.distributed.Client</code>.</p> <code>None</code> <p>Examples:</p> <p>Using a temporary local dask cluster: <pre><code>from prefect import flow\nfrom prefect_dask.task_runners import DaskTaskRunner\n@flow(task_runner=DaskTaskRunner)\ndef my_flow():\n    ...\n</code></pre></p> <p>Using a temporary cluster running elsewhere. Any Dask cluster class should work, here we use dask-cloudprovider: <pre><code>DaskTaskRunner(\n    cluster_class=\"dask_cloudprovider.FargateCluster\",\n    cluster_kwargs={\n        \"image\": \"prefecthq/prefect:latest\",\n        \"n_workers\": 5,\n    },\n)\n</code></pre></p> <p>Connecting to an existing dask cluster: <pre><code>DaskTaskRunner(address=\"192.0.2.255:8786\")\n</code></pre></p> Source code in <code>prefect_dask/task_runners.py</code> <pre><code>class DaskTaskRunner(BaseTaskRunner):\n\"\"\"\n    A parallel task_runner that submits tasks to the `dask.distributed` scheduler.\n    By default a temporary `distributed.LocalCluster` is created (and\n    subsequently torn down) within the `start()` contextmanager. To use a\n    different cluster class (e.g.\n    [`dask_kubernetes.KubeCluster`](https://kubernetes.dask.org/)), you can\n    specify `cluster_class`/`cluster_kwargs`.\n\n    Alternatively, if you already have a dask cluster running, you can provide\n    the address of the scheduler via the `address` kwarg.\n\n    !!! warning \"Multiprocessing safety\"\n        Note that, because the `DaskTaskRunner` uses multiprocessing, calls to flows\n        in scripts must be guarded with `if __name__ == \"__main__\":` or warnings will\n        be displayed.\n\n    Args:\n        address (string, optional): Address of a currently running dask\n            scheduler; if one is not provided, a temporary cluster will be\n            created in `DaskTaskRunner.start()`.  Defaults to `None`.\n        cluster_class (string or callable, optional): The cluster class to use\n            when creating a temporary dask cluster. Can be either the full\n            class name (e.g. `\"distributed.LocalCluster\"`), or the class itself.\n        cluster_kwargs (dict, optional): Additional kwargs to pass to the\n            `cluster_class` when creating a temporary dask cluster.\n        adapt_kwargs (dict, optional): Additional kwargs to pass to `cluster.adapt`\n            when creating a temporary dask cluster. Note that adaptive scaling\n            is only enabled if `adapt_kwargs` are provided.\n        client_kwargs (dict, optional): Additional kwargs to use when creating a\n            [`dask.distributed.Client`](https://distributed.dask.org/en/latest/api.html#client).\n\n    Examples:\n        Using a temporary local dask cluster:\n        ```python\n        from prefect import flow\n        from prefect_dask.task_runners import DaskTaskRunner\n        @flow(task_runner=DaskTaskRunner)\n        def my_flow():\n            ...\n        ```\n\n        Using a temporary cluster running elsewhere. Any Dask cluster class should\n        work, here we use [dask-cloudprovider](https://cloudprovider.dask.org):\n        ```python\n        DaskTaskRunner(\n            cluster_class=\"dask_cloudprovider.FargateCluster\",\n            cluster_kwargs={\n                \"image\": \"prefecthq/prefect:latest\",\n                \"n_workers\": 5,\n            },\n        )\n        ```\n\n        Connecting to an existing dask cluster:\n        ```python\n        DaskTaskRunner(address=\"192.0.2.255:8786\")\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        address: str = None,\n        cluster_class: Union[str, Callable] = None,\n        cluster_kwargs: dict = None,\n        adapt_kwargs: dict = None,\n        client_kwargs: dict = None,\n    ):\n        # Validate settings and infer defaults\n        if address:\n            if cluster_class or cluster_kwargs or adapt_kwargs:\n                raise ValueError(\n                    \"Cannot specify `address` and \"\n                    \"`cluster_class`/`cluster_kwargs`/`adapt_kwargs`\"\n                )\n        else:\n            if isinstance(cluster_class, str):\n                cluster_class = from_qualified_name(cluster_class)\n            else:\n                cluster_class = cluster_class\n\n        # Create a copies of incoming kwargs since we may mutate them\n        cluster_kwargs = cluster_kwargs.copy() if cluster_kwargs else {}\n        adapt_kwargs = adapt_kwargs.copy() if adapt_kwargs else {}\n        client_kwargs = client_kwargs.copy() if client_kwargs else {}\n\n        # Update kwargs defaults\n        client_kwargs.setdefault(\"set_as_default\", False)\n\n        # The user cannot specify async/sync themselves\n        if \"asynchronous\" in client_kwargs:\n            raise ValueError(\n                \"`client_kwargs` cannot set `asynchronous`. \"\n                \"This option is managed by Prefect.\"\n            )\n        if \"asynchronous\" in cluster_kwargs:\n            raise ValueError(\n                \"`cluster_kwargs` cannot set `asynchronous`. \"\n                \"This option is managed by Prefect.\"\n            )\n\n        # Store settings\n        self.address = address\n        self.cluster_class = cluster_class\n        self.cluster_kwargs = cluster_kwargs\n        self.adapt_kwargs = adapt_kwargs\n        self.client_kwargs = client_kwargs\n\n        # Runtime attributes\n        self._client: \"distributed.Client\" = None\n        self._cluster: \"distributed.deploy.Cluster\" = None\n        self._dask_futures: Dict[str, \"distributed.Future\"] = {}\n\n        super().__init__()\n\n    @property\n    def concurrency_type(self) -&gt; TaskConcurrencyType:\n        return (\n            TaskConcurrencyType.PARALLEL\n            if self.cluster_kwargs.get(\"processes\")\n            else TaskConcurrencyType.CONCURRENT\n        )\n\n    async def submit(\n        self,\n        key: UUID,\n        call: Callable[..., Awaitable[State[R]]],\n    ) -&gt; None:\n        if not self._started:\n            raise RuntimeError(\n                \"The task runner must be started before submitting work.\"\n            )\n\n        # unpack the upstream call in order to cast Prefect futures to Dask futures\n        # where possible to optimize Dask task scheduling\n        call_kwargs = self._optimize_futures(call.keywords)\n\n        if \"task_run\" in call_kwargs:\n            task_run = call_kwargs[\"task_run\"]\n            flow_run = FlowRunContext.get().flow_run\n            # Dask displays the text up to the first '-' as the name; the task run key\n            # should include the task run name for readability in the Dask console.\n            # For cases where the task run fails and reruns for a retried flow run,\n            # the flow run count is included so that the new key will not match\n            # the failed run's key, therefore not retrieving from the Dask cache.\n            dask_key = f\"{task_run.name}-{task_run.id.hex}-{flow_run.run_count}\"\n        else:\n            dask_key = key\n\n        self._dask_futures[key] = self._client.submit(\n            call.func,\n            key=dask_key,\n            # Dask defaults to treating functions are pure, but we set this here for\n            # explicit expectations. If this task run is submitted to Dask twice, the\n            # result of the first run should be returned. Subsequent runs would return\n            # `Abort` exceptions if they were submitted again.\n            pure=True,\n            **call_kwargs,\n        )\n\n    def _get_dask_future(self, key: UUID) -&gt; \"distributed.Future\":\n\"\"\"\n        Retrieve the dask future corresponding to a Prefect future.\n        The Dask future is for the `run_fn`, which should return a `State`.\n        \"\"\"\n        return self._dask_futures[key]\n\n    def _optimize_futures(self, expr):\n        def visit_fn(expr):\n            if isinstance(expr, PrefectFuture):\n                dask_future = self._dask_futures.get(expr.key)\n                if dask_future is not None:\n                    return dask_future\n            # Fallback to return the expression unaltered\n            return expr\n\n        return visit_collection(expr, visit_fn=visit_fn, return_data=True)\n\n    async def wait(self, key: UUID, timeout: float = None) -&gt; Optional[State]:\n        future = self._get_dask_future(key)\n        try:\n            return await future.result(timeout=timeout)\n        except distributed.TimeoutError:\n            return None\n        except BaseException as exc:\n            return await exception_to_crashed_state(exc)\n\n    async def _start(self, exit_stack: AsyncExitStack):\n\"\"\"\n        Start the task runner and prep for context exit.\n        - Creates a cluster if an external address is not set.\n        - Creates a client to connect to the cluster.\n        - Pushes a call to wait for all running futures to complete on exit.\n        \"\"\"\n        if self.address:\n            self.logger.info(\n                f\"Connecting to an existing Dask cluster at {self.address}\"\n            )\n            self._connect_to = self.address\n        else:\n            self.cluster_class = self.cluster_class or distributed.LocalCluster\n\n            self.logger.info(\n                f\"Creating a new Dask cluster with \"\n                f\"`{to_qualified_name(self.cluster_class)}`\"\n            )\n            self._connect_to = self._cluster = await exit_stack.enter_async_context(\n                self.cluster_class(asynchronous=True, **self.cluster_kwargs)\n            )\n            if self.adapt_kwargs:\n                self._cluster.adapt(**self.adapt_kwargs)\n\n        self._client = await exit_stack.enter_async_context(\n            distributed.Client(\n                self._connect_to, asynchronous=True, **self.client_kwargs\n            )\n        )\n\n        if self._client.dashboard_link:\n            self.logger.info(\n                f\"The Dask dashboard is available at {self._client.dashboard_link}\",\n            )\n\n    def __getstate__(self):\n\"\"\"\n        Allow the `DaskTaskRunner` to be serialized by dropping\n        the `distributed.Client`, which contains locks.\n        Must be deserialized on a dask worker.\n        \"\"\"\n        data = self.__dict__.copy()\n        data.update({k: None for k in {\"_client\", \"_cluster\", \"_connect_to\"}})\n        return data\n\n    def __setstate__(self, data: dict):\n\"\"\"\n        Restore the `distributed.Client` by loading the client on a dask worker.\n        \"\"\"\n        self.__dict__.update(data)\n        self._client = distributed.get_client()\n</code></pre>"},{"location":"task_runners/#prefect_dask.task_runners.DaskTaskRunner-functions","title":"Functions","text":""},{"location":"task_runners/#prefect_dask.task_runners.DaskTaskRunner.__getstate__","title":"<code>__getstate__</code>","text":"<p>Allow the <code>DaskTaskRunner</code> to be serialized by dropping the <code>distributed.Client</code>, which contains locks. Must be deserialized on a dask worker.</p> Source code in <code>prefect_dask/task_runners.py</code> <pre><code>def __getstate__(self):\n\"\"\"\n    Allow the `DaskTaskRunner` to be serialized by dropping\n    the `distributed.Client`, which contains locks.\n    Must be deserialized on a dask worker.\n    \"\"\"\n    data = self.__dict__.copy()\n    data.update({k: None for k in {\"_client\", \"_cluster\", \"_connect_to\"}})\n    return data\n</code></pre>"},{"location":"task_runners/#prefect_dask.task_runners.DaskTaskRunner.__setstate__","title":"<code>__setstate__</code>","text":"<p>Restore the <code>distributed.Client</code> by loading the client on a dask worker.</p> Source code in <code>prefect_dask/task_runners.py</code> <pre><code>def __setstate__(self, data: dict):\n\"\"\"\n    Restore the `distributed.Client` by loading the client on a dask worker.\n    \"\"\"\n    self.__dict__.update(data)\n    self._client = distributed.get_client()\n</code></pre>"},{"location":"usage_guide/","title":"Usage Guide","text":"<p>Below is a guide on how to use <code>prefect-dask</code> effectively.</p>"},{"location":"usage_guide/#running-tasks-on-dask","title":"Running tasks on Dask","text":"<p>The <code>DaskTaskRunner</code> is a parallel task runner that submits tasks to the <code>dask.distributed</code> scheduler. </p> <p>By default, a temporary Dask cluster is created for the duration of the flow run.</p> <p>For example, this flow counts up to 10 in parallel (note that the output is not sequential).</p> <pre><code>import time\n\nfrom prefect import flow, task\nfrom prefect_dask import DaskTaskRunner\n\n@task\ndef shout(number):\n    time.sleep(0.5)\n    print(f\"#{number}\")\n\n@flow(task_runner=DaskTaskRunner)\ndef count_to(highest_number):\n    for number in range(highest_number):\n        shout.submit(number)\n\nif __name__ == \"__main__\":\n    count_to(10)\n\n# outputs\n#3\n#7\n#2\n#6\n#4\n#0\n#1\n#5\n#8\n#9\n</code></pre> <p>If you already have a Dask cluster running, either local or cloud hosted, you can provide the connection URL via an <code>address</code> argument.</p> <p>To configure your flow to use the <code>DaskTaskRunner</code>:</p> <ol> <li>Make sure the <code>prefect-dask</code> collection is installed as described earlier: <code>pip install prefect-dask</code>.</li> <li>In your flow code, import <code>DaskTaskRunner</code> from <code>prefect_dask.task_runners</code>.</li> <li>Assign it as the task runner when the flow is defined using the <code>task_runner=DaskTaskRunner</code> argument.</li> </ol> <p>For example, this flow uses the <code>DaskTaskRunner</code> configured to access an existing Dask cluster at <code>http://my-dask-cluster</code>.</p> <pre><code>from prefect import flow\nfrom prefect_dask.task_runners import DaskTaskRunner\n\n@flow(task_runner=DaskTaskRunner(address=\"http://my-dask-cluster\"))\ndef my_flow():\n    ...\n</code></pre> <p><code>DaskTaskRunner</code> accepts the following optional parameters:</p> Parameter Description address Address of a currently running Dask scheduler. cluster_class The cluster class to use when creating a temporary Dask cluster. It can be either the full class name (for example, <code>\"distributed.LocalCluster\"</code>), or the class itself. cluster_kwargs Additional kwargs to pass to the <code>cluster_class</code> when creating a temporary Dask cluster. adapt_kwargs Additional kwargs to pass to <code>cluster.adapt</code> when creating a temporary Dask cluster. Note that adaptive scaling is only enabled if <code>adapt_kwargs</code> are provided. client_kwargs Additional kwargs to use when creating a <code>dask.distributed.Client</code>. <p>Multiprocessing safety</p> <p>Note that, because the <code>DaskTaskRunner</code> uses multiprocessing, calls to flows in scripts must be guarded with <code>if __name__ == \"__main__\":</code> or you will encounter  warnings and errors.</p> <p>If you don't provide the <code>address</code> of a Dask scheduler, Prefect creates a temporary local cluster automatically. The number of workers used is based on the number of cores available to your execution environment. The default provides a mix of processes and threads that should work well for most workloads. If you want to specify this explicitly, you can pass values for <code>n_workers</code> or <code>threads_per_worker</code> to <code>cluster_kwargs</code>.</p> <pre><code># Use 4 worker processes, each with 2 threads\nDaskTaskRunner(\n    cluster_kwargs={\"n_workers\": 4, \"threads_per_worker\": 2}\n)\n</code></pre>"},{"location":"usage_guide/#distributing-dask-collections-across-workers","title":"Distributing Dask collections across workers","text":"<p>If you use a Dask collection, such as a <code>dask.DataFrame</code> or <code>dask.Bag</code>, to distribute the work across workers and achieve parallel computations, use one of the context managers <code>get_dask_client</code> or <code>get_async_dask_client</code>:</p> <pre><code>import dask\nfrom prefect import flow, task\nfrom prefect_dask import DaskTaskRunner, get_dask_client\n\n@task\ndef compute_task():\n    with get_dask_client() as client:\n        df = dask.datasets.timeseries(\"2000\", \"2001\", partition_freq=\"4w\")\n        summary_df = df.describe().compute()\n    return summary_df\n\n@flow(task_runner=DaskTaskRunner())\ndef dask_flow():\n    prefect_future = compute_task.submit()\n    return prefect_future.result()\n\ndask_flow()\n</code></pre> <p>The context managers can be used the same way in both <code>flow</code> run contexts and <code>task</code> run contexts.</p> <p>Resolving futures in sync client</p> <p>Note, by default, <code>dask_collection.compute()</code> returns concrete values while <code>client.compute(dask_collection)</code> returns Dask Futures. Therefore, if you call <code>client.compute</code>, you must resolve all futures before exiting out of the context manager by either:</p> <ol> <li> <p>setting <code>sync=True</code> <pre><code>with get_dask_client() as client:\n    df = dask.datasets.timeseries(\"2000\", \"2001\", partition_freq=\"4w\")\n    summary_df = client.compute(df.describe(), sync=True)\n</code></pre></p> </li> <li> <p>calling <code>result()</code> <pre><code>with get_dask_client() as client:\n    df = dask.datasets.timeseries(\"2000\", \"2001\", partition_freq=\"4w\")\n    summary_df = client.compute(df.describe()).result()\n</code></pre> For more information, visit the docs on Waiting on Futures.</p> </li> </ol> <p>There is also an equivalent context manager for asynchronous tasks and flows: <code>get_async_dask_client</code>.</p> <pre><code>import asyncio\n\nimport dask\nfrom prefect import flow, task\nfrom prefect_dask import DaskTaskRunner, get_async_dask_client\n\n@task\nasync def compute_task():\n    async with get_async_dask_client() as client:\n        df = dask.datasets.timeseries(\"2000\", \"2001\", partition_freq=\"4w\")\n        summary_df = await client.compute(df.describe())\n    return summary_df\n\n@flow(task_runner=DaskTaskRunner())\nasync def dask_flow():\n    prefect_future = await compute_task.submit()\n    return await prefect_future.result()\n\nasyncio.run(dask_flow())\n</code></pre> <p>Resolving futures in async client</p> <p>With the async client, you do not need to set <code>sync=True</code> or call <code>result()</code>.</p> <p>However you must <code>await client.compute(dask_collection)</code> before exiting out of the context manager.</p> <p>To invoke <code>compute</code> from the Dask collection, set <code>sync=False</code> and call <code>result()</code> before exiting out of the context manager: <code>await dask_collection.compute(sync=False)</code>.</p>"},{"location":"usage_guide/#using-a-temporary-cluster","title":"Using a temporary cluster","text":"<p>The <code>DaskTaskRunner</code> is capable of creating a temporary cluster using any of Dask's cluster-manager options. This can be useful when you want each flow run to have its own Dask cluster, allowing for per-flow adaptive scaling.</p> <p>To configure, you need to provide a <code>cluster_class</code>. This can be:</p> <ul> <li>A string specifying the import path to the cluster class (for example, <code>\"dask_cloudprovider.aws.FargateCluster\"</code>)</li> <li>The cluster class itself</li> <li>A function for creating a custom cluster</li> </ul> <p>You can also configure <code>cluster_kwargs</code>, which takes a dictionary of keyword arguments to pass to <code>cluster_class</code> when starting the flow run.</p> <p>For example, to configure a flow to use a temporary <code>dask_cloudprovider.aws.FargateCluster</code> with 4 workers running with an image named <code>my-prefect-image</code>:</p> <pre><code>DaskTaskRunner(\n    cluster_class=\"dask_cloudprovider.aws.FargateCluster\",\n    cluster_kwargs={\"n_workers\": 4, \"image\": \"my-prefect-image\"},\n)\n</code></pre>"},{"location":"usage_guide/#connecting-to-an-existing-cluster","title":"Connecting to an existing cluster","text":"<p>Multiple Prefect flow runs can all use the same existing Dask cluster. You might manage a single long-running Dask cluster (maybe using the Dask Helm Chart) and configure flows to connect to it during execution. This has a few downsides when compared to using a temporary cluster (as described above):</p> <ul> <li>All workers in the cluster must have dependencies installed for all flows you intend to run.</li> <li>Multiple flow runs may compete for resources. Dask tries to do a good job sharing resources between tasks, but you may still run into issues.</li> </ul> <p>That said, you may prefer managing a single long-running cluster. </p> <p>To configure a <code>DaskTaskRunner</code> to connect to an existing cluster, pass in the address of the scheduler to the <code>address</code> argument:</p> <pre><code># Connect to an existing cluster running at a specified address\nDaskTaskRunner(address=\"tcp://...\")\n</code></pre>"},{"location":"usage_guide/#adaptive-scaling","title":"Adaptive scaling","text":"<p>One nice feature of using a <code>DaskTaskRunner</code> is the ability to scale adaptively to the workload. Instead of specifying <code>n_workers</code> as a fixed number, this lets you specify a minimum and maximum number of workers to use, and the dask cluster will scale up and down as needed.</p> <p>To do this, you can pass <code>adapt_kwargs</code> to <code>DaskTaskRunner</code>. This takes the following fields:</p> <ul> <li><code>maximum</code> (<code>int</code> or <code>None</code>, optional): the maximum number of workers to scale to. Set to <code>None</code> for no maximum.</li> <li><code>minimum</code> (<code>int</code> or <code>None</code>, optional): the minimum number of workers to scale to. Set to <code>None</code> for no minimum.</li> </ul> <p>For example, here we configure a flow to run on a <code>FargateCluster</code> scaling up to at most 10 workers.</p> <pre><code>DaskTaskRunner(\n    cluster_class=\"dask_cloudprovider.aws.FargateCluster\",\n    adapt_kwargs={\"maximum\": 10}\n)\n</code></pre>"},{"location":"usage_guide/#dask-annotations","title":"Dask annotations","text":"<p>Dask annotations can be used to further control the behavior of tasks.</p> <p>For example, we can set the priority of tasks in the Dask scheduler:</p> <pre><code>import dask\nfrom prefect import flow, task\nfrom prefect_dask.task_runners import DaskTaskRunner\n\n@task\ndef show(x):\n    print(x)\n\n\n@flow(task_runner=DaskTaskRunner())\ndef my_flow():\n    with dask.annotate(priority=-10):\n        future = show(1)  # low priority task\n\n    with dask.annotate(priority=10):\n        future = show(2)  # high priority task\n</code></pre> <p>Another common use case is resource annotations:</p> <pre><code>import dask\nfrom prefect import flow, task\nfrom prefect_dask.task_runners import DaskTaskRunner\n\n@task\ndef show(x):\n    print(x)\n\n# Create a `LocalCluster` with some resource annotations\n# Annotations are abstract in dask and not inferred from your system.\n# Here, we claim that our system has 1 GPU and 1 process available per worker\n@flow(\n    task_runner=DaskTaskRunner(\n        cluster_kwargs={\"n_workers\": 1, \"resources\": {\"GPU\": 1, \"process\": 1}}\n    )\n)\ndef my_flow():\n    with dask.annotate(resources={'GPU': 1}):\n        future = show(0)  # this task requires 1 GPU resource on a worker\n\n    with dask.annotate(resources={'process': 1}):\n        # These tasks each require 1 process on a worker; because we've \n        # specified that our cluster has 1 process per worker and 1 worker,\n        # these tasks will run sequentially\n        future = show(1)\n        future = show(2)\n        future = show(3)\n</code></pre> <p>For more tips on how to use tasks and flows in a Collection, check out Using Collections!</p>"},{"location":"utils/","title":"Utils","text":""},{"location":"utils/#prefect_dask.utils","title":"<code>prefect_dask.utils</code>","text":"<p>Utils to use alongside prefect-dask.</p>"},{"location":"utils/#prefect_dask.utils-functions","title":"Functions","text":""},{"location":"utils/#prefect_dask.utils.get_async_dask_client","title":"<code>get_async_dask_client</code>  <code>async</code>","text":"<p>Yields a temporary asynchronous dask client; this is useful for parallelizing operations on dask collections, such as a <code>dask.DataFrame</code> or <code>dask.Bag</code>.</p> <p>Without invoking this, workers do not automatically get a client to connect to the full cluster. Therefore, it will attempt perform work within the worker itself serially, and potentially overwhelming the single worker.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>Optional[Union[int, float, str, timedelta]]</code> <p>Timeout after which to error out; has no effect in flow run contexts because the client has already started; Defaults to the <code>distributed.comm.timeouts.connect</code> configuration value.</p> <code>None</code> <code>client_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>distributed.Client</code>, and overwrites inherited keyword arguments from the task runner, if any.</p> <code>{}</code> <p>Yields:</p> Type Description <code>Client</code> <p>A temporary asynchronous dask client.</p> <p>Examples:</p> <p>Use <code>get_async_dask_client</code> to distribute work across workers. <pre><code>import dask\nfrom prefect import flow, task\nfrom prefect_dask import DaskTaskRunner, get_async_dask_client\n\n@task\nasync def compute_task():\n    async with get_async_dask_client(timeout=\"120s\") as client:\n        df = dask.datasets.timeseries(\"2000\", \"2001\", partition_freq=\"4w\")\n        summary_df = await client.compute(df.describe())\n    return summary_df\n\n@flow(task_runner=DaskTaskRunner())\nasync def dask_flow():\n    prefect_future = await compute_task.submit()\n    return await prefect_future.result()\n\nasyncio.run(dask_flow())\n</code></pre></p> Source code in <code>prefect_dask/utils.py</code> <pre><code>@asynccontextmanager\nasync def get_async_dask_client(\n    timeout: Optional[Union[int, float, str, timedelta]] = None,\n    **client_kwargs: Dict[str, Any],\n) -&gt; Client:\n\"\"\"\n    Yields a temporary asynchronous dask client; this is useful\n    for parallelizing operations on dask collections,\n    such as a `dask.DataFrame` or `dask.Bag`.\n\n    Without invoking this, workers do not automatically get a client to connect\n    to the full cluster. Therefore, it will attempt perform work within the\n    worker itself serially, and potentially overwhelming the single worker.\n\n    Args:\n        timeout: Timeout after which to error out; has no effect in\n            flow run contexts because the client has already started;\n            Defaults to the `distributed.comm.timeouts.connect`\n            configuration value.\n        client_kwargs: Additional keyword arguments to pass to\n            `distributed.Client`, and overwrites inherited keyword arguments\n            from the task runner, if any.\n\n    Yields:\n        A temporary asynchronous dask client.\n\n    Examples:\n        Use `get_async_dask_client` to distribute work across workers.\n        ```python\n        import dask\n        from prefect import flow, task\n        from prefect_dask import DaskTaskRunner, get_async_dask_client\n\n        @task\n        async def compute_task():\n            async with get_async_dask_client(timeout=\"120s\") as client:\n                df = dask.datasets.timeseries(\"2000\", \"2001\", partition_freq=\"4w\")\n                summary_df = await client.compute(df.describe())\n            return summary_df\n\n        @flow(task_runner=DaskTaskRunner())\n        async def dask_flow():\n            prefect_future = await compute_task.submit()\n            return await prefect_future.result()\n\n        asyncio.run(dask_flow())\n        ```\n    \"\"\"\n    client_kwargs = _generate_client_kwargs(\n        async_client=True, timeout=timeout, **client_kwargs\n    )\n    async with Client(**client_kwargs) as client:\n        yield client\n</code></pre>"},{"location":"utils/#prefect_dask.utils.get_dask_client","title":"<code>get_dask_client</code>","text":"<p>Yields a temporary synchronous dask client; this is useful for parallelizing operations on dask collections, such as a <code>dask.DataFrame</code> or <code>dask.Bag</code>.</p> <p>Without invoking this, workers do not automatically get a client to connect to the full cluster. Therefore, it will attempt perform work within the worker itself serially, and potentially overwhelming the single worker.</p> <p>When in an async context, we recommend using <code>get_async_dask_client</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>Optional[Union[int, float, str, timedelta]]</code> <p>Timeout after which to error out; has no effect in flow run contexts because the client has already started; Defaults to the <code>distributed.comm.timeouts.connect</code> configuration value.</p> <code>None</code> <code>client_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>distributed.Client</code>, and overwrites inherited keyword arguments from the task runner, if any.</p> <code>{}</code> <p>Yields:</p> Type Description <code>Client</code> <p>A temporary synchronous dask client.</p> <p>Examples:</p> <p>Use <code>get_dask_client</code> to distribute work across workers. <pre><code>import dask\nfrom prefect import flow, task\nfrom prefect_dask import DaskTaskRunner, get_dask_client\n\n@task\ndef compute_task():\n    with get_dask_client(timeout=\"120s\") as client:\n        df = dask.datasets.timeseries(\"2000\", \"2001\", partition_freq=\"4w\")\n        summary_df = client.compute(df.describe()).result()\n    return summary_df\n\n@flow(task_runner=DaskTaskRunner())\ndef dask_flow():\n    prefect_future = compute_task.submit()\n    return prefect_future.result()\n\ndask_flow()\n</code></pre></p> Source code in <code>prefect_dask/utils.py</code> <pre><code>@contextmanager\ndef get_dask_client(\n    timeout: Optional[Union[int, float, str, timedelta]] = None,\n    **client_kwargs: Dict[str, Any],\n) -&gt; Client:\n\"\"\"\n    Yields a temporary synchronous dask client; this is useful\n    for parallelizing operations on dask collections,\n    such as a `dask.DataFrame` or `dask.Bag`.\n\n    Without invoking this, workers do not automatically get a client to connect\n    to the full cluster. Therefore, it will attempt perform work within the\n    worker itself serially, and potentially overwhelming the single worker.\n\n    When in an async context, we recommend using `get_async_dask_client` instead.\n\n    Args:\n        timeout: Timeout after which to error out; has no effect in\n            flow run contexts because the client has already started;\n            Defaults to the `distributed.comm.timeouts.connect`\n            configuration value.\n        client_kwargs: Additional keyword arguments to pass to\n            `distributed.Client`, and overwrites inherited keyword arguments\n            from the task runner, if any.\n\n    Yields:\n        A temporary synchronous dask client.\n\n    Examples:\n        Use `get_dask_client` to distribute work across workers.\n        ```python\n        import dask\n        from prefect import flow, task\n        from prefect_dask import DaskTaskRunner, get_dask_client\n\n        @task\n        def compute_task():\n            with get_dask_client(timeout=\"120s\") as client:\n                df = dask.datasets.timeseries(\"2000\", \"2001\", partition_freq=\"4w\")\n                summary_df = client.compute(df.describe()).result()\n            return summary_df\n\n        @flow(task_runner=DaskTaskRunner())\n        def dask_flow():\n            prefect_future = compute_task.submit()\n            return prefect_future.result()\n\n        dask_flow()\n        ```\n    \"\"\"\n    client_kwargs = _generate_client_kwargs(\n        async_client=False, timeout=timeout, **client_kwargs\n    )\n    with Client(**client_kwargs) as client:\n        yield client\n</code></pre>"}]}